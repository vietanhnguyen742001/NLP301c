{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "Chuyển đổi văn bản thành các \"token\" nhỏ hơn (có thể là từ, cụm từ, hoặc câu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'test', '.']\n",
      "['hello world.', 'This is an NLTK test.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "\n",
    "text = \"hello world. This is an NLTK test. \"\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "print(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech Tagging\n",
    "Gán nhãn từ loại cho mỗi token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Think\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')   \n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'JJ'), ('world', 'NN'), ('.', '.'), ('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('NLTK', 'JJ'), ('test', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "words_tagged = pos_tag(words)\n",
    "print(words_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER)\n",
    "Nhận diện và phân loại các thực thể đặc biệt như tên người, tổ chức, địa điểm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  hello/JJ\n",
      "  world/NN\n",
      "  ./.\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (ORGANIZATION NLTK/JJ)\n",
      "  test/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "ner = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization và Stemming\n",
    "Chuẩn hóa các từ về dạng gốc của chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'This', 'is', 'an', 'NLTK', 'test', '.']\n",
      "['hello', 'world', '.', 'thi', 'is', 'an', 'nltk', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(lemmatized)#Lemmatization thích hợp cho các ứng dụng cần đến độ chính xác cao\n",
    "print(stemmed)#stemming có thể được sử dụng cho các ứng dụng cần đến tốc độ xử lý nhanh và không cần quá chính xác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Word Probabilities\n",
    "Xác suất của từ là cơ sở cho rất nhiều ứng dụng trong NLP, từ việc tạo mô hình ngôn ngữ cho đến việc phân tích cảm xúc và phân loại văn bản. Trong NLP, xác suất của một từ thường được xác định dựa trên tần suất xuất hiện của nó trong một tập dữ liệu lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002743669320379616\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng thư viện\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Lấy một mẫu văn bản từ corpus của Gutenberg\n",
    "sample_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "tokens = word_tokenize(sample_text)\n",
    "\n",
    "# Tính tần suất xuất hiện của mỗi từ\n",
    "freq_dist = FreqDist(tokens)\n",
    "\n",
    "# Xác suất của một từ cụ thể\n",
    "word = 'Macbeth'\n",
    "word_probability = freq_dist[word] / len(tokens)\n",
    "print(word_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code tay\n",
    "from collections import Counter\n",
    "\n",
    "# Giả sử `tokens` là một danh sách các từ trong văn bản của bạn\n",
    "word_counts = Counter(tokens)\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "# Xác suất của một từ cụ thể\n",
    "word_probability = word_counts[word] / total_words\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Dynamic Programming\n",
    "(DP) là một kỹ thuật lập trình để giải quyết các bài toán tối ưu hóa bằng cách phân rã chúng thành các bài toán con nhỏ hơn và lưu trữ kết quả của các bài toán con để tránh tính toán lặp lại. DP thường được sử dụng trong các vấn đề như tìm kiếm đường đi tối ưu, tối ưu hóa các chuỗi, và tối thiểu hóa khoảng cách chỉnh sửa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fibonacci(n, memo={}):\n",
    "    if n in memo:\n",
    "        return memo[n]\n",
    "    if n <= 2:\n",
    "        return 1\n",
    "    memo[n]=fibonacci(n-1, memo) + fibonacci(n-2, memo)\n",
    "    return memo[n]\n",
    "fibonacci(10)\n",
    "#0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3. Minimum Edit Distance\n",
    "Khoảng cách chỉnh sửa tối thiểu (Minimum Edit Distance) là số lượng thao tác tối thiểu cần thiết để biến đổi một chuỗi thành chuỗi khác, với các thao tác bao gồm chèn, xóa, và thay thế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm này sử dụng một thuật toán quy hoạch động để tính toán khoảng cách chỉnh sửa. Nó tạo ra một bảng dp kích thước (n+1) x (m+1), trong đó n là độ dài của source và m là độ dài của target. Mỗi ô dp[i][j] trong bảng biểu diễn khoảng cách chỉnh sửa tối thiểu giữa source[:i] và target[:j].\n",
    "\n",
    "Đầu tiên, hàm khởi tạo hàng đầu tiên và cột đầu tiên của bảng, biểu diễn trường hợp một trong hai chuỗi là chuỗi rỗng.\n",
    "\n",
    "Sau đó, hàm điền vào phần còn lại của bảng. Nếu source[i-1] bằng target[j-1], thì dp[i][j] bằng dp[i-1][j-1] vì không cần thao tác nào. Nếu không, dp[i][j] bằng 1 cộng với giá trị nhỏ nhất của dp[i-1][j] (xóa), dp[i][j-1] (chèn), và dp[i-1][j-1] (thay thế).\n",
    "\n",
    "Cuối cùng, hàm trả về dp[n][m], là khoảng cách chỉnh sửa tối thiểu giữa source và target.\n",
    "\n",
    "Trong trường hợp của bạn, min_edit_distance('intention','execution') trả về 5, có nghĩa là cần ít nhất 5 thao tác để biến đổi 'intention' thành 'execution'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def min_edit_distance(source, target):\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(m+1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            if source[i-1]== target[j-1]:\n",
    "                dp[i][j]=dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j]=1 + min(dp[i-1][j], dp[i][j-1],dp[i-1][j-1])\n",
    "    return dp[n][m]\n",
    "min_edit_distance('intention','execution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4. Autocorrect\n",
    "Tính năng tự động sửa lỗi thường dựa vào khoảng cách chỉnh sửa tối thiểu để tìm từ gần giống nhất với từ bị lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giả sử `vocabulary` là một danh sách các từ hợp lệ và `word` là từ cần sửa\n",
    "def autocorrect(word, vocabulary):\n",
    "    min_distance = float('inf')\n",
    "    correct_word = word\n",
    "    for v in vocabulary:\n",
    "        distance = min_edit_distance(word, v)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            correct_word = v\n",
    "    return correct_word\n",
    "autocorrect(word=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
